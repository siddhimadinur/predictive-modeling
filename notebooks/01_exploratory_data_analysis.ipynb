{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the house price dataset to understand patterns, relationships, and data quality issues that will inform our modeling approach.\n",
    "\n",
    "## Objectives\n",
    "1. Load and validate the dataset\n",
    "2. Analyze data quality and missing values\n",
    "3. Explore target variable distribution\n",
    "4. Investigate feature relationships and correlations\n",
    "5. Identify patterns and insights for feature engineering\n",
    "6. Generate actionable recommendations for data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_loader import load_data_with_fallback, validate_dataset\n",
    "from src.eda import EDAAnalyzer, compare_train_test_distributions\n",
    "from src.data_quality import DataQualityAssessor\n",
    "from src.utils import print_data_info\n",
    "\n",
    "# Configure display and plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìä EDA Environment Setup Complete\")\n",
    "print(\"üìÅ Loading house price data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with fallback to sample data if Kaggle dataset not available\n",
    "train_data, test_data = load_data_with_fallback()\n",
    "\n",
    "# Validate dataset structure\n",
    "is_valid = validate_dataset(train_data, test_data)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"\\n‚úÖ Dataset validation successful!\")\n",
    "    print(f\"üìà Training data shape: {train_data.shape}\")\n",
    "    print(f\"üìä Test data shape: {test_data.shape}\")\n",
    "    print(f\"üîç Total features: {train_data.shape[1] - 1}\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset validation failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data quality assessor\n",
    "quality_assessor = DataQualityAssessor(train_data, \"Training Data\")\n",
    "\n",
    "# Generate comprehensive quality report\n",
    "quality_report = quality_assessor.generate_quality_report()\n",
    "\n",
    "# Print quality summary\n",
    "quality_assessor.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = quality_report['missing_values']\n",
    "\n",
    "print(\"üìä MISSING VALUES DETAILED ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  ‚Ä¢ Total missing values: {missing_analysis['summary']['total_missing_values']:,}\")\n",
    "print(f\"  ‚Ä¢ Overall missing percentage: {missing_analysis['summary']['missing_percentage_overall']:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Columns with missing data: {missing_analysis['summary']['columns_with_missing']}\")\n",
    "print(f\"  ‚Ä¢ Complete columns: {missing_analysis['summary']['complete_columns']}\")\n",
    "\n",
    "print(f\"\\nBy Severity:\")\n",
    "for severity, columns in missing_analysis['by_severity'].items():\n",
    "    if columns:\n",
    "        print(f\"  ‚Ä¢ {severity.replace('_', ' ').title()}: {len(columns)} columns\")\n",
    "        if len(columns) <= 5:\n",
    "            print(f\"    {columns}\")\n",
    "        else:\n",
    "            print(f\"    {columns[:5]}... (and {len(columns)-5} more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EDA analyzer\n",
    "eda_analyzer = EDAAnalyzer(train_data, test_data)\n",
    "\n",
    "# Analyze target variable\n",
    "target_analysis = eda_analyzer.analyze_target_variable()\n",
    "\n",
    "print(\"üéØ TARGET VARIABLE ANALYSIS (SalePrice)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'error' not in target_analysis:\n",
    "    print(f\"\\nDescriptive Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Count: {target_analysis['count']:,}\")\n",
    "    print(f\"  ‚Ä¢ Mean: ${target_analysis['mean']:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Median: ${target_analysis['median']:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Standard Deviation: ${target_analysis['std']:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Range: ${target_analysis['min']:,.0f} - ${target_analysis['max']:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Skewness: {target_analysis['skewness']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Kurtosis: {target_analysis['kurtosis']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nPrice Distribution:\")\n",
    "    ranges = target_analysis['price_ranges']\n",
    "    total = sum(ranges.values())\n",
    "    for range_name, count in ranges.items():\n",
    "        percentage = count / total * 100\n",
    "        print(f\"  ‚Ä¢ {range_name.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOutlier Analysis:\")\n",
    "    outliers = target_analysis['outliers']\n",
    "    print(f\"  ‚Ä¢ Outlier count: {outliers['outlier_count']} ({outliers['outlier_percentage']:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Outlier bounds: ${outliers['lower_bound']:,.0f} - ${outliers['upper_bound']:,.0f}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error in target analysis: {target_analysis['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target variable distribution\n",
    "eda_analyzer.plot_target_distribution(figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "eda_analyzer.plot_missing_values(figsize=(15, 10), top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value patterns analysis\n",
    "missing_patterns = quality_report['missing_values']['patterns']\n",
    "\n",
    "print(\"üîç MISSING VALUE PATTERNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nRow-level Analysis:\")\n",
    "print(f\"  ‚Ä¢ Rows with missing data: {missing_patterns['rows_with_missing']:,}\")\n",
    "print(f\"  ‚Ä¢ Complete rows: {missing_patterns['rows_complete']:,}\")\n",
    "print(f\"  ‚Ä¢ Complete rows percentage: {missing_patterns['complete_rows_percentage']:.1f}%\")\n",
    "\n",
    "if missing_patterns['highly_correlated_missing']:\n",
    "    print(f\"\\nHighly Correlated Missing Values:\")\n",
    "    for corr_pair in missing_patterns['highly_correlated_missing'][:5]:\n",
    "        print(f\"  ‚Ä¢ {corr_pair['feature_1']} ‚Üî {corr_pair['feature_2']}: {corr_pair['correlation']:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No highly correlated missing value patterns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations with target variable\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    top_correlations = eda_analyzer.target_correlation(top_n=15)\n",
    "    \n",
    "    print(\"üîó TOP FEATURES CORRELATED WITH SALEPRICE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for idx, row in top_correlations.iterrows():\n",
    "        direction = \"üìà\" if row['correlation'] > 0 else \"üìâ\"\n",
    "        print(f\"{idx+1:2d}. {direction} {row['feature']:<20} : {row['correlation']:>7.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SalePrice column not found for correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap for top features\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    eda_analyzer.plot_correlation_heatmap(figsize=(14, 12), top_n=15)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot plot correlation heatmap without SalePrice column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features\n",
    "numerical_stats = eda_analyzer.analyze_numerical_features()\n",
    "\n",
    "print(\"üî¢ NUMERICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal numerical features: {len(numerical_stats)}\")\n",
    "\n",
    "# Features with high skewness\n",
    "high_skew_features = [(name, stats['skewness']) for name, stats in numerical_stats.items() \n",
    "                      if abs(stats['skewness']) > 1]\n",
    "high_skew_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "if high_skew_features:\n",
    "    print(f\"\\nHighly Skewed Features (|skewness| > 1):\")\n",
    "    for feature, skewness in high_skew_features[:10]:\n",
    "        direction = \"right\" if skewness > 0 else \"left\"\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {skewness:>7.2f} ({direction} skewed)\")\n",
    "\n",
    "# Features with many zeros\n",
    "high_zero_features = [(name, stats['zeros_count'], stats['zeros_count']/len(train_data)*100) \n",
    "                      for name, stats in numerical_stats.items() \n",
    "                      if stats['zeros_count']/len(train_data) > 0.3]\n",
    "high_zero_features.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "if high_zero_features:\n",
    "    print(f\"\\nFeatures with Many Zeros (>30%):\")\n",
    "    for feature, count, percentage in high_zero_features[:10]:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {count:>5} ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key numerical features\n",
    "key_numerical_features = ['GrLivArea', 'TotalBsmtSF', 'LotArea', 'YearBuilt', 'OverallQual']\n",
    "existing_features = [f for f in key_numerical_features if f in train_data.columns]\n",
    "\n",
    "if existing_features:\n",
    "    n_features = len(existing_features)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, feature in enumerate(existing_features):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Plot histogram\n",
    "        train_data[feature].hist(bins=30, ax=ax, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_title(f'{feature} Distribution')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add statistics\n",
    "        if feature in numerical_stats:\n",
    "            stats = numerical_stats[feature]\n",
    "            ax.axvline(stats['mean'], color='red', linestyle='--', alpha=0.7, label=f\"Mean: {stats['mean']:.0f}\")\n",
    "            ax.axvline(stats['median'], color='green', linestyle='--', alpha=0.7, label=f\"Median: {stats['median']:.0f}\")\n",
    "            ax.legend()\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(existing_features), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Key numerical features not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "categorical_stats = eda_analyzer.analyze_categorical_features()\n",
    "\n",
    "print(\"üìù CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal categorical features: {len(categorical_stats)}\")\n",
    "\n",
    "# High cardinality features\n",
    "high_cardinality = [(name, stats['unique_values']) for name, stats in categorical_stats.items() \n",
    "                    if stats['unique_values'] > 20]\n",
    "high_cardinality.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if high_cardinality:\n",
    "    print(f\"\\nHigh Cardinality Features (>20 unique values):\")\n",
    "    for feature, unique_count in high_cardinality:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {unique_count:>3} unique values\")\n",
    "\n",
    "# Low cardinality features\n",
    "low_cardinality = [(name, stats['unique_values']) for name, stats in categorical_stats.items() \n",
    "                   if stats['unique_values'] <= 5]\n",
    "low_cardinality.sort(key=lambda x: x[1])\n",
    "\n",
    "if low_cardinality:\n",
    "    print(f\"\\nLow Cardinality Features (‚â§5 unique values):\")\n",
    "    for feature, unique_count in low_cardinality:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {unique_count:>3} unique values\")\n",
    "\n",
    "# Features with missing values\n",
    "categorical_missing = [(name, stats['missing_count'], stats['missing_count']/len(train_data)*100) \n",
    "                       for name, stats in categorical_stats.items() \n",
    "                       if stats['missing_count'] > 0]\n",
    "categorical_missing.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "if categorical_missing:\n",
    "    print(f\"\\nCategorical Features with Missing Values:\")\n",
    "    for feature, count, percentage in categorical_missing[:10]:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {count:>5} ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between key categorical features and target\n",
    "key_categorical_features = ['Neighborhood', 'OverallQual', 'ExterQual', 'KitchenQual']\n",
    "existing_cat_features = [f for f in key_categorical_features if f in train_data.columns]\n",
    "\n",
    "if existing_cat_features and 'SalePrice' in train_data.columns:\n",
    "    n_features = min(4, len(existing_cat_features))  # Limit to 4 for better visualization\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(existing_cat_features[:n_features]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create boxplot\n",
    "        sns.boxplot(data=train_data, x=feature, y='SalePrice', ax=ax)\n",
    "        ax.set_title(f'SalePrice by {feature}', fontsize=14)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Format y-axis to show prices in thousands\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, 4):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    if 'SalePrice' not in train_data.columns:\n",
    "        print(\"‚ö†Ô∏è SalePrice column not found for categorical analysis\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Key categorical features not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Relationships and Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for key feature relationships\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    feature_pairs = [\n",
    "        ('GrLivArea', 'SalePrice'),\n",
    "        ('TotalBsmtSF', 'SalePrice'),\n",
    "        ('YearBuilt', 'SalePrice'),\n",
    "        ('OverallQual', 'SalePrice')\n",
    "    ]\n",
    "    \n",
    "    # Filter to existing features\n",
    "    existing_pairs = [(f1, f2) for f1, f2 in feature_pairs \n",
    "                      if f1 in train_data.columns and f2 in train_data.columns]\n",
    "    \n",
    "    if existing_pairs:\n",
    "        n_pairs = len(existing_pairs)\n",
    "        n_cols = 2\n",
    "        n_rows = (n_pairs + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, (feature1, feature2) in enumerate(existing_pairs):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Create scatter plot\n",
    "            ax.scatter(train_data[feature1], train_data[feature2], alpha=0.6, s=20)\n",
    "            ax.set_xlabel(feature1)\n",
    "            ax.set_ylabel(feature2)\n",
    "            ax.set_title(f'{feature1} vs {feature2}')\n",
    "            \n",
    "            # Add correlation coefficient\n",
    "            if train_data[feature1].dtype in ['int64', 'float64'] and train_data[feature2].dtype in ['int64', 'float64']:\n",
    "                corr = train_data[feature1].corr(train_data[feature2])\n",
    "                ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                       transform=ax.transAxes, \n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(existing_pairs), n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No suitable feature pairs found for scatter plots\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SalePrice column not found for relationship analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data distributions\n",
    "distribution_analysis = eda_analyzer.data_distribution_analysis()\n",
    "\n",
    "print(\"üìä DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group features by distribution type\n",
    "distribution_groups = {}\n",
    "for feature, analysis in distribution_analysis.items():\n",
    "    dist_type = analysis['distribution_type']\n",
    "    if dist_type not in distribution_groups:\n",
    "        distribution_groups[dist_type] = []\n",
    "    distribution_groups[dist_type].append((feature, analysis['skewness']))\n",
    "\n",
    "for dist_type, features in distribution_groups.items():\n",
    "    print(f\"\\n{dist_type.replace('_', ' ').title()} ({len(features)} features):\")\n",
    "    for feature, skewness in sorted(features, key=lambda x: abs(x[1]), reverse=True)[:5]:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} (skewness: {skewness:>6.2f})\")\n",
    "    if len(features) > 5:\n",
    "        print(f\"    ... and {len(features) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers in key numerical features\n",
    "outlier_analysis = quality_assessor.assess_outliers()\n",
    "\n",
    "print(\"üéØ OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Features with high outlier percentages\n",
    "high_outlier_features = []\n",
    "for feature, analysis in outlier_analysis.items():\n",
    "    outlier_pct = analysis['iqr_method']['outlier_percentage']\n",
    "    if outlier_pct > 5:  # More than 5% outliers\n",
    "        high_outlier_features.append((feature, outlier_pct, analysis['iqr_method']['outlier_count']))\n",
    "\n",
    "high_outlier_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if high_outlier_features:\n",
    "    print(f\"\\nFeatures with High Outlier Rates (>5%):\")\n",
    "    for feature, pct, count in high_outlier_features[:10]:\n",
    "        print(f\"  ‚Ä¢ {feature:<20} : {count:>4} outliers ({pct:>5.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No features with high outlier rates detected\")\n",
    "\n",
    "# Summary statistics\n",
    "total_outliers = sum(analysis['iqr_method']['outlier_count'] for analysis in outlier_analysis.values())\n",
    "avg_outlier_pct = np.mean([analysis['iqr_method']['outlier_percentage'] for analysis in outlier_analysis.values()])\n",
    "\n",
    "print(f\"\\nOutlier Summary:\")\n",
    "print(f\"  ‚Ä¢ Total outliers detected: {total_outliers:,}\")\n",
    "print(f\"  ‚Ä¢ Average outlier rate: {avg_outlier_pct:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Features analyzed: {len(outlier_analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train-Test Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions between train and test sets for key features\n",
    "comparison_features = ['GrLivArea', 'YearBuilt', 'OverallQual', 'Neighborhood']\n",
    "existing_comparison_features = [f for f in comparison_features if f in train_data.columns and f in test_data.columns]\n",
    "\n",
    "if existing_comparison_features:\n",
    "    print(\"üîç TRAIN-TEST DISTRIBUTION COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for feature in existing_comparison_features[:4]:  # Limit to 4 features\n",
    "        print(f\"\\nComparing {feature} distribution...\")\n",
    "        compare_train_test_distributions(train_data, test_data, feature, figsize=(12, 6))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No suitable features found for train-test comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive EDA Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive EDA report\n",
    "print(\"üìã GENERATING COMPREHENSIVE EDA REPORT...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eda_report = eda_analyzer.generate_eda_report()\n",
    "\n",
    "# Print detailed summary\n",
    "eda_analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile key insights and recommendations\n",
    "print(\"üí° KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data Quality Insights\n",
    "print(\"\\nüîç DATA QUALITY INSIGHTS:\")\n",
    "if 'overall_quality' in quality_report:\n",
    "    quality_score = quality_report['overall_quality']\n",
    "    print(f\"  ‚Ä¢ Overall Quality Score: {quality_score['overall_score']:.1f}/100 (Grade: {quality_score['grade']})\")\n",
    "    print(f\"  ‚Ä¢ Assessment: {quality_score['interpretation']}\")\n",
    "\n",
    "missing_recs = quality_report['missing_values']['recommendations']\n",
    "if missing_recs:\n",
    "    print(\"\\nüìä MISSING VALUES RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(missing_recs, 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "\n",
    "# Feature Engineering Insights\n",
    "print(\"\\nüîß FEATURE ENGINEERING OPPORTUNITIES:\")\n",
    "\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    top_corr = eda_analyzer.target_correlation(5)\n",
    "    print(\"  ‚Ä¢ Focus on top correlated features for modeling:\")\n",
    "    for _, row in top_corr.iterrows():\n",
    "        print(f\"    - {row['feature']} (correlation: {row['correlation']:.3f})\")\n",
    "\n",
    "if high_skew_features:\n",
    "    print(\"  ‚Ä¢ Consider log transformation for highly skewed features:\")\n",
    "    for feature, skewness in high_skew_features[:5]:\n",
    "        print(f\"    - {feature} (skewness: {skewness:.2f})\")\n",
    "\n",
    "if high_zero_features:\n",
    "    print(\"  ‚Ä¢ Consider binary encoding for features with many zeros:\")\n",
    "    for feature, count, percentage in high_zero_features[:3]:\n",
    "        print(f\"    - {feature} ({percentage:.1f}% zeros)\")\n",
    "\n",
    "# Modeling Recommendations\n",
    "print(\"\\nü§ñ MODELING RECOMMENDATIONS:\")\n",
    "print(\"  ‚Ä¢ Apply robust scaling for features with outliers\")\n",
    "print(\"  ‚Ä¢ Consider ensemble methods to handle feature complexity\")\n",
    "print(\"  ‚Ä¢ Use cross-validation to assess model stability\")\n",
    "print(\"  ‚Ä¢ Monitor for overfitting given feature diversity\")\n",
    "\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    target_stats = eda_analyzer.analyze_target_variable()\n",
    "    if target_stats['skewness'] > 1:\n",
    "        print(\"  ‚Ä¢ Consider log transformation of target variable (SalePrice)\")\n",
    "\n",
    "print(\"\\n‚úÖ EDA ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nüöÄ Ready to proceed to Phase 3: Data Processing & Feature Engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key results for use in subsequent phases\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare summary for export\n",
    "eda_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'train_shape': train_data.shape,\n",
    "        'test_shape': test_data.shape,\n",
    "        'numerical_features': len(train_data.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical_features': len(train_data.select_dtypes(include=['object']).columns)\n",
    "    },\n",
    "    'data_quality_grade': quality_report.get('overall_quality', {}).get('grade', 'N/A'),\n",
    "    'missing_values_summary': quality_report['missing_values']['summary'],\n",
    "    'recommendations': {\n",
    "        'missing_values': missing_recs,\n",
    "        'high_skew_features': [f[0] for f in high_skew_features[:10]] if 'high_skew_features' in locals() else [],\n",
    "        'high_zero_features': [f[0] for f in high_zero_features[:5]] if 'high_zero_features' in locals() else []\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'SalePrice' in train_data.columns:\n",
    "    top_corr = eda_analyzer.target_correlation(10)\n",
    "    eda_summary['top_correlated_features'] = {\n",
    "        row['feature']: row['correlation'] for _, row in top_corr.iterrows()\n",
    "    }\n",
    "\n",
    "# Save to file\n",
    "output_path = '../data/processed/eda_summary.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(eda_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ EDA summary saved to: {output_path}\")\n",
    "print(\"\\nüìã Summary includes:\")\n",
    "print(\"  ‚Ä¢ Dataset information and statistics\")\n",
    "print(\"  ‚Ä¢ Data quality assessment grade\")\n",
    "print(\"  ‚Ä¢ Missing values analysis\")\n",
    "print(\"  ‚Ä¢ Feature correlation rankings\")\n",
    "print(\"  ‚Ä¢ Recommendations for preprocessing\")\n",
    "\n",
    "print(\"\\nüéâ Phase 2: Data Acquisition & Exploration - COMPLETED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}