{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing - Model Development & Training\n",
    "\n",
    "This notebook implements comprehensive model training and evaluation for California housing price prediction using our engineered features from Phase 3.\n",
    "\n",
    "## Objectives\n",
    "1. Load processed California housing data\n",
    "2. Train multiple regression models (Linear, Ridge, Lasso, Random Forest, XGBoost)\n",
    "3. Perform hyperparameter tuning for optimal performance\n",
    "4. Evaluate and compare model performance\n",
    "5. Analyze feature importance and model interpretability\n",
    "6. Select best model for deployment\n",
    "7. Save trained models for web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model_training import CaliforniaHousingModelTrainer\n",
    "from src.model_validation import CaliforniaHousingModelValidator\n",
    "from src.data_pipeline import CaliforniaHousingPipeline\n",
    "\n",
    "# Import model classes\n",
    "from src.models.linear_models import *\n",
    "from src.models.ensemble_models import *\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üè† California Housing Model Development Environment Setup Complete\")\n",
    "print(f\"üìÖ Training session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed California housing data\n",
    "print(\"üìä Loading processed California housing data...\")\n",
    "\n",
    "try:\n",
    "    # Try to load previously processed data\n",
    "    pipeline = CaliforniaHousingPipeline(pd.DataFrame(), pd.DataFrame())\n",
    "    datasets = pipeline.load_processed_data()\n",
    "\n",
    "    if 'X_train' in datasets and 'X_val' in datasets:\n",
    "        X_train = datasets['X_train']\n",
    "        X_val = datasets['X_val']\n",
    "        y_train = datasets['y_train']\n",
    "        y_val = datasets['y_val']\n",
    "        print(\"‚úÖ Processed data loaded successfully from Phase 3\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Processed data not found\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Processed data not found. Running preprocessing pipeline...\")\n",
    "    # Load and process data\n",
    "    from src.data_loader import load_data_with_fallback\n",
    "    \n",
    "    train_data, test_data = load_data_with_fallback()\n",
    "    pipeline = CaliforniaHousingPipeline(train_data, test_data)\n",
    "    results = pipeline.run_pipeline(save_processed=True)\n",
    "    \n",
    "    X_train = pipeline.X_train\n",
    "    X_val = pipeline.X_val\n",
    "    y_train = pipeline.y_train\n",
    "    y_val = pipeline.y_val\n",
    "    \n",
    "    print(\"‚úÖ Data processed and ready for training\")\n",
    "\n",
    "print(f\"\\nüìà California Housing Dataset Ready:\")\n",
    "print(f\"  ‚Ä¢ Training features: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ Validation features: {X_val.shape}\")\n",
    "print(f\"  ‚Ä¢ Training target: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ Validation target: {y_val.shape}\")\n",
    "print(f\"  ‚Ä¢ Feature names (first 10): {list(X_train.columns[:10])}\")\n",
    "print(f\"  ‚Ä¢ Target statistics: Mean=${y_train.mean():,.0f}, Range=${y_train.min():,.0f}-${y_train.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "print(\"üöÄ Initializing California Housing Model Trainer...\")\n",
    "trainer = CaliforniaHousingModelTrainer(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Register all models optimized for California housing\n",
    "trainer.register_california_housing_models()\n",
    "\n",
    "print(f\"\\nüìù Registered Models for California Housing:\")\n",
    "for i, model_name in enumerate(trainer.models.keys(), 1):\n",
    "    model_class, params = trainer.models[model_name]\n",
    "    print(f\"  {i}. {model_name} ({model_class.__name__})\")\n",
    "    if params:\n",
    "        key_params = {k: v for k, v in params.items() if k in ['alpha', 'n_estimators', 'learning_rate', 'max_depth']}\n",
    "        if key_params:\n",
    "            print(f\"     Parameters: {key_params}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Cross-validation folds: 5\")\n",
    "print(f\"  ‚Ä¢ Hyperparameter tuning: Will be applied to selected models\")\n",
    "print(f\"  ‚Ä¢ Evaluation metrics: RMSE, R¬≤, MAE, MAPE\")\n",
    "print(f\"  ‚Ä¢ Target: California house values (median_house_value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training - Phase 1 (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with default parameters first (faster baseline)\n",
    "print(\"üîß Phase 1: Training models with default parameters...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train without hyperparameter tuning for quick baseline\n",
    "trained_models = trainer.train_all_models(tune_hyperparameters=False, cv_folds=5)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 Training Completed: {len(trained_models)} models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model performance\n",
    "print(\"üìä Evaluating baseline model performance...\")\n",
    "\n",
    "baseline_evaluation = trainer.evaluate_models()\n",
    "print(\"\\nüìà BASELINE MODEL PERFORMANCE:\")\n",
    "print(baseline_evaluation[['model', 'val_r2', 'val_rmse', 'val_mae', 'rmse_ratio']].round(4))\n",
    "\n",
    "# Get baseline best model\n",
    "baseline_best_name, baseline_best_model = trainer.get_best_model()\n",
    "baseline_best_metrics = baseline_evaluation[baseline_evaluation['model'] == baseline_best_name].iloc[0]\n",
    "\n",
    "print(f\"\\nüèÜ Baseline Best Model: {baseline_best_name}\")\n",
    "print(f\"  ‚Ä¢ Validation R¬≤: {baseline_best_metrics['val_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Validation RMSE: ${baseline_best_metrics['val_rmse']:,.0f}\")\n",
    "print(f\"  ‚Ä¢ RMSE as % of mean price: {baseline_best_metrics.get('rmse_relative_pct', 0):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline model comparison\n",
    "print(\"üìä Creating baseline model comparison visualizations...\")\n",
    "trainer.plot_model_comparison(figsize=(16, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training - Phase 2 (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for top performing models\n",
    "print(\"üîç Phase 2: Hyperparameter tuning for top models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select top 3 models for tuning based on baseline performance\n",
    "top_3_models = baseline_evaluation.head(3)['model'].tolist()\n",
    "print(f\"\\nüéØ Selected models for hyperparameter tuning: {top_3_models}\")\n",
    "\n",
    "# Create new trainer for tuned models\n",
    "tuning_trainer = CaliforniaHousingModelTrainer(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Register only the top models for tuning\n",
    "for model_name in top_3_models:\n",
    "    if model_name in trainer.models:\n",
    "        model_class, kwargs = trainer.models[model_name]\n",
    "        tuning_trainer.register_model(model_class, f'{model_name}_tuned', **kwargs)\n",
    "\n",
    "print(f\"\\nüîÑ Training {len(tuning_trainer.models)} models with hyperparameter tuning...\")\n",
    "tuned_models = tuning_trainer.train_all_models(tune_hyperparameters=True, cv_folds=5)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2 Tuning Completed: {len(tuned_models)} models tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned vs baseline models\n",
    "if tuned_models:\n",
    "    tuned_evaluation = tuning_trainer.evaluate_models()\n",
    "    \n",
    "    print(\"\\nüìä TUNED MODELS PERFORMANCE:\")\n",
    "    print(tuned_evaluation[['model', 'val_r2', 'val_rmse', 'val_mae', 'rmse_ratio']].round(4))\n",
    "    \n",
    "    # Performance improvement analysis\n",
    "    print(f\"\\nüìà BASELINE vs TUNED COMPARISON:\")\n",
    "    print(f\"{'Model':<20} {'Baseline R¬≤':<12} {'Tuned R¬≤':<12} {'Improvement':<12}\")\n",
    "    print(f\"-\"*60)\n",
    "    \n",
    "    for original_name in top_3_models:\n",
    "        tuned_name = f'{original_name}_tuned'\n",
    "        \n",
    "        if tuned_name in tuned_evaluation['model'].values:\n",
    "            baseline_r2 = baseline_evaluation[baseline_evaluation['model'] == original_name]['val_r2'].iloc[0]\n",
    "            tuned_r2 = tuned_evaluation[tuned_evaluation['model'] == tuned_name]['val_r2'].iloc[0]\n",
    "            improvement = tuned_r2 - baseline_r2\n",
    "            \n",
    "            print(f\"{original_name:<20} {baseline_r2:<12.4f} {tuned_r2:<12.4f} {improvement:+.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No tuned models available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all models for final evaluation\n",
    "all_models = {**trained_models, **tuned_models}\n",
    "\n",
    "print(f\"üîç Comprehensive validation of {len(all_models)} models...\")\n",
    "\n",
    "# Initialize validator\n",
    "validator = CaliforniaHousingModelValidator(all_models, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Create comprehensive performance plots\n",
    "validator.plot_model_performance(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model comparison and selection\n",
    "final_comparison = validator.compare_models()\n",
    "\n",
    "print(\"üèÜ FINAL MODEL COMPARISON - CALIFORNIA HOUSING\")\n",
    "print(\"=\"*70)\n",
    "print(final_comparison[['model', 'val_r2', 'val_rmse', 'val_mae', 'rmse_ratio', 'rmse_as_pct_of_mean_price']].round(4))\n",
    "\n",
    "# Select final best model\n",
    "final_best_name = final_comparison.iloc[0]['model']\n",
    "final_best_model = all_models[final_best_name]\n",
    "final_best_metrics = final_comparison.iloc[0]\n",
    "\n",
    "print(f\"\\nüéØ FINAL BEST MODEL: {final_best_name}\")\n",
    "print(f\"  ‚Ä¢ Validation R¬≤: {final_best_metrics['val_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Validation RMSE: ${final_best_metrics['val_rmse']:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Validation MAE: ${final_best_metrics['val_mae']:,.0f}\")\n",
    "print(f\"  ‚Ä¢ RMSE as % of mean house value: {final_best_metrics['rmse_as_pct_of_mean_price']:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Predictions within $20K: {final_best_metrics.get('prediction_accuracy_within_20k', 'N/A')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Model Deep Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best performing model\n",
    "print(f\"üîç DEEP ANALYSIS: {final_best_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print detailed performance summary\n",
    "final_best_model.print_performance_summary()\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nüìä Feature Importance Analysis:\")\n",
    "importance_df = final_best_model.get_feature_importance()\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(f\"\\nüîù Top 15 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):\n",
    "        print(f\"  {i:2d}. {row['feature']:<30}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    final_best_model.plot_feature_importance(top_n=20, figsize=(12, 10))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs actual visualization for best model\n",
    "print(f\"üìà Prediction Analysis for {final_best_name}...\")\n",
    "\n",
    "# Plot predictions vs actual\n",
    "final_best_model.plot_predictions(\n",
    "    X_val, y_val, \n",
    "    title=f'{final_best_name} - California Housing Validation Set Predictions',\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "\n",
    "# Detailed residual analysis\n",
    "validator.plot_residual_analysis(final_best_name, figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for best model\n",
    "print(f\"üìö Learning Curves Analysis for {final_best_name}...\")\n",
    "validator.plot_learning_curves(final_best_name, figsize=(12, 6))\n",
    "\n",
    "# Prediction confidence analysis\n",
    "confidence_analysis = validator.create_prediction_confidence_analysis(final_best_name)\n",
    "\n",
    "print(f\"\\nüéØ PREDICTION CONFIDENCE BY HOUSE VALUE RANGE:\")\n",
    "print(f\"{'Price Range':<15} {'Sample Count':<12} {'Mean Error':<12} {'Accuracy (¬±$20K)':<15}\")\n",
    "print(f\"-\"*70)\n",
    "\n",
    "for range_name, analysis in confidence_analysis.items():\n",
    "    print(f\"{range_name:<15} {analysis['sample_count']:<12} \"\n",
    "          f\"${analysis['mean_error']:<11,.0f} {analysis['accuracy_within_20k']:<14.1f}%\")\n",
    "    print(f\"{'':>15} {analysis['price_range']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretability for California housing\n",
    "print(\"üß† MODEL INTERPRETABILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Feature importance across different model types\n",
    "feature_importance_comparison = {}\n",
    "\n",
    "# Get feature importance from different model types\n",
    "model_types_for_interpretation = ['linear', 'ridge', 'lasso', 'random_forest']\n",
    "available_models = [name for name in model_types_for_interpretation if name in all_models]\n",
    "\n",
    "for model_name in available_models:\n",
    "    model = all_models[model_name]\n",
    "    importance_df = model.get_feature_importance()\n",
    "    \n",
    "    if importance_df is not None:\n",
    "        # Get top 10 features for this model\n",
    "        top_features = importance_df.head(10)\n",
    "        feature_importance_comparison[model_name] = dict(zip(top_features['feature'], top_features['importance']))\n",
    "\n",
    "# Find most consistently important features\n",
    "if feature_importance_comparison:\n",
    "    all_important_features = set()\n",
    "    for features in feature_importance_comparison.values():\n",
    "        all_important_features.update(features.keys())\n",
    "    \n",
    "    # Count how many models consider each feature important\n",
    "    feature_consensus = {}\n",
    "    for feature in all_important_features:\n",
    "        count = sum(1 for features in feature_importance_comparison.values() if feature in features)\n",
    "        avg_importance = np.mean([features.get(feature, 0) for features in feature_importance_comparison.values()])\n",
    "        feature_consensus[feature] = {'model_count': count, 'avg_importance': avg_importance}\n",
    "    \n",
    "    # Sort by consensus (model count, then average importance)\n",
    "    consensus_ranking = sorted(feature_consensus.items(), \n",
    "                              key=lambda x: (x[1]['model_count'], x[1]['avg_importance']), \n",
    "                              reverse=True)\n",
    "    \n",
    "    print(f\"\\nüéØ FEATURE IMPORTANCE CONSENSUS (Top 15):\")\n",
    "    print(f\"{'Feature':<30} {'Models':<8} {'Avg Importance':<15}\")\n",
    "    print(f\"-\"*60)\n",
    "    \n",
    "    for feature, stats in consensus_ranking[:15]:\n",
    "        print(f\"{feature:<30} {stats['model_count']}/{len(feature_importance_comparison):<7} {stats['avg_importance']:<15.4f}\")\n",
    "\n",
    "# California housing specific insights\n",
    "print(f\"\\nüè† CALIFORNIA HOUSING INSIGHTS:\")\n",
    "if importance_df is not None:\n",
    "    top_features = importance_df.head(5)['feature'].tolist()\n",
    "    \n",
    "    housing_insights = {\n",
    "        'median_income': \"üí∞ Income is the strongest predictor of housing values in CA\",\n",
    "        'total_rooms': \"üè† Larger properties command higher prices\",\n",
    "        'housing_median_age': \"üìÖ Newer homes are generally more valuable\",\n",
    "        'longitude': \"üó∫Ô∏è East-West location affects prices (proximity to coast)\",\n",
    "        'latitude': \"üó∫Ô∏è North-South location indicates different CA markets\",\n",
    "        'population': \"üë• Population density impacts housing demand\",\n",
    "        'ocean_proximity': \"üåä Proximity to ocean significantly affects values\",\n",
    "        'rooms_per_household': \"üìè Housing density is a key value driver\",\n",
    "        'distance_to_Los_Angeles': \"üèôÔ∏è Distance from major cities affects prices\",\n",
    "        'distance_to_San_Francisco': \"üèôÔ∏è SF proximity is a major price factor\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nKey predictive insights from top features:\")\n",
    "    for feature in top_features:\n",
    "        for key, insight in housing_insights.items():\n",
    "            if key in feature.lower():\n",
    "                print(f\"  ‚Ä¢ {insight}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {feature}: Important engineered feature for CA housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection combining baseline and tuned models\n",
    "print(\"üèÜ FINAL MODEL SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all models for final comparison\n",
    "final_validator = CaliforniaHousingModelValidator(all_models, X_train, y_train, X_val, y_val)\n",
    "final_comparison = final_validator.compare_models()\n",
    "\n",
    "print(\"\\nüìä FINAL MODEL RANKING:\")\n",
    "print(final_comparison[['model', 'val_r2', 'val_rmse', 'rmse_as_pct_of_mean_price', 'prediction_accuracy_within_20k']].round(4))\n",
    "\n",
    "# Select champion model\n",
    "champion_model_name = final_comparison.iloc[0]['model']\n",
    "champion_model = all_models[champion_model_name]\n",
    "champion_metrics = final_comparison.iloc[0]\n",
    "\n",
    "print(f\"\\nü•á CHAMPION MODEL: {champion_model_name}\")\n",
    "print(f\"  ‚Ä¢ Final Validation R¬≤: {champion_metrics['val_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Final Validation RMSE: ${champion_metrics['val_rmse']:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Error as % of mean house value: {champion_metrics['rmse_as_pct_of_mean_price']:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Predictions within $20K: {champion_metrics.get('prediction_accuracy_within_20k', 'N/A'):.1f}%\")\n",
    "\n",
    "# Model performance interpretation\n",
    "rmse_pct = champion_metrics['rmse_as_pct_of_mean_price']\n",
    "if rmse_pct < 10:\n",
    "    performance_grade = \"Excellent\"\n",
    "elif rmse_pct < 15:\n",
    "    performance_grade = \"Very Good\"\n",
    "elif rmse_pct < 20:\n",
    "    performance_grade = \"Good\"\n",
    "elif rmse_pct < 30:\n",
    "    performance_grade = \"Fair\"\n",
    "else:\n",
    "    performance_grade = \"Needs Improvement\"\n",
    "\n",
    "print(f\"\\nüìà Model Performance Grade: {performance_grade}\")\n",
    "print(f\"üí° Interpretation: The model's predictions are typically within {rmse_pct:.1f}% of the actual house value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Champion Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of the champion model\n",
    "print(f\"üèÜ CHAMPION MODEL DEEP DIVE: {champion_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Detailed validation\n",
    "champion_validation = validator.validate_single_model(champion_model)\n",
    "\n",
    "# Plot comprehensive analysis\n",
    "champion_model.plot_predictions(X_val, y_val, \n",
    "                               title=f'{champion_model_name} - California Housing Predictions',\n",
    "                               figsize=(12, 8))\n",
    "\n",
    "# Residual analysis\n",
    "champion_model.plot_residuals(X_val, y_val, figsize=(18, 6))\n",
    "\n",
    "# Feature importance for champion\n",
    "if champion_model.get_feature_importance() is not None:\n",
    "    champion_model.plot_feature_importance(top_n=20, figsize=(12, 10))\n",
    "\n",
    "# Learning curves\n",
    "validator.plot_learning_curves(champion_model_name, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison Across Price Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance across different house value ranges\n",
    "print(\"üè† MODEL PERFORMANCE BY HOUSE VALUE RANGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create price range analysis for top 3 models\n",
    "top_3_final = final_comparison.head(3)['model'].tolist()\n",
    "\n",
    "price_ranges = pd.qcut(y_val, q=5, labels=['Low ($)', 'Low-Med ($)', 'Medium ($)', 'Med-High ($)', 'High ($)'])\n",
    "\n",
    "range_analysis = {}\n",
    "for model_name in top_3_final:\n",
    "    model = all_models[model_name]\n",
    "    predictions = model.predict(X_val)\n",
    "    \n",
    "    range_performance = {}\n",
    "    for price_range in price_ranges.cat.categories:\n",
    "        mask = price_ranges == price_range\n",
    "        if mask.sum() > 0:\n",
    "            range_actual = y_val[mask]\n",
    "            range_pred = predictions[mask]\n",
    "            \n",
    "            range_performance[price_range] = {\n",
    "                'r2': r2_score(range_actual, range_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(range_actual, range_pred)),\n",
    "                'sample_count': len(range_actual),\n",
    "                'mean_actual': range_actual.mean()\n",
    "            }\n",
    "    \n",
    "    range_analysis[model_name] = range_performance\n",
    "\n",
    "# Display results\n",
    "for model_name, ranges in range_analysis.items():\n",
    "    print(f\"\\nüìä {model_name} Performance by Price Range:\")\n",
    "    print(f\"{'Range':<12} {'Samples':<8} {'Mean Price':<12} {'R¬≤':<8} {'RMSE':<10}\")\n",
    "    print(f\"-\"*60)\n",
    "    \n",
    "    for range_name, metrics in ranges.items():\n",
    "        print(f\"{range_name:<12} {metrics['sample_count']:<8} \"\n",
    "              f\"${metrics['mean_actual']:<11,.0f} {metrics['r2']:<8.3f} \"\n",
    "              f\"${metrics['rmse']:<9,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "print(\"üíæ SAVING TRAINED MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save baseline models\n",
    "print(\"\\nüì¶ Saving baseline models...\")\n",
    "trainer.save_models()\n",
    "\n",
    "# Save tuned models\n",
    "if tuned_models:\n",
    "    print(\"\\nüì¶ Saving tuned models...\")\n",
    "    tuning_trainer.save_models()\n",
    "\n",
    "# Save champion model separately for easy access\n",
    "from config.settings import MODELS_DIR\n",
    "champion_path = MODELS_DIR / 'champion_california_housing_model.pkl'\n",
    "champion_model.save_model(champion_path)\n",
    "\n",
    "print(f\"\\nüèÜ Champion model saved separately: {champion_path}\")\n",
    "\n",
    "# Save final comparison results\n",
    "final_comparison.to_csv(MODELS_DIR / 'final_california_housing_model_comparison.csv', index=False)\n",
    "\n",
    "# Create deployment summary\n",
    "deployment_summary = {\n",
    "    'champion_model': champion_model_name,\n",
    "    'champion_r2': float(champion_metrics['val_r2']),\n",
    "    'champion_rmse': float(champion_metrics['val_rmse']),\n",
    "    'dataset': 'california_housing',\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'feature_count': X_train.shape[1],\n",
    "    'target_column': 'median_house_value',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'all_models_trained': list(all_models.keys())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(MODELS_DIR / 'deployment_summary.json', 'w') as f:\n",
    "    json.dump(deployment_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ All models and results saved to: {MODELS_DIR}\")\n",
    "print(f\"üìã Files saved:\")\n",
    "import os\n",
    "for file in os.listdir(MODELS_DIR):\n",
    "    if file.endswith(('.pkl', '.csv', '.json')):\n",
    "        print(f\"  ‚Ä¢ {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive training report\n",
    "print(\"üìã GENERATING FINAL TRAINING REPORT...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate baseline training report\n",
    "baseline_report = trainer.generate_training_report()\n",
    "print(baseline_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate validation report\n",
    "validation_report = final_validator.generate_validation_report()\n",
    "print(validation_report)\n",
    "\n",
    "print(f\"\\nüéâ PHASE 4: MODEL DEVELOPMENT & TRAINING - COMPLETED!\")\n",
    "print(f\"\\nüìä Final Results Summary:\")\n",
    "print(f\"  ‚Ä¢ Champion Model: {champion_model_name}\")\n",
    "print(f\"  ‚Ä¢ Best Validation R¬≤: {champion_metrics['val_r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Best Validation RMSE: ${champion_metrics['val_rmse']:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Models Trained: {len(all_models)}\")\n",
    "print(f\"  ‚Ä¢ Features Used: {X_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Dataset: California Housing ({len(X_train):,} properties)\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Phase 5: Web Application & Deployment!\")\n",
    "print(f\"üí° The champion model is ready to predict California housing prices!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}