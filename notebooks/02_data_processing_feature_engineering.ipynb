{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing - Data Processing & Feature Engineering\n",
    "\n",
    "This notebook implements comprehensive data preprocessing and feature engineering for the California housing dataset, preparing the data for machine learning model training.\n",
    "\n",
    "## Objectives\n",
    "1. Load and clean the California housing dataset\n",
    "2. Handle any missing values and data quality issues\n",
    "3. Engineer meaningful features specific to housing data\n",
    "4. Create geographic, density, and interaction features\n",
    "5. Scale and encode features appropriately\n",
    "6. Split data into training and validation sets\n",
    "7. Save processed data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_loader import load_data_with_fallback, validate_dataset\n",
    "from src.data_pipeline import CaliforniaHousingPipeline\n",
    "from src.data_cleaning import DataCleaner\n",
    "from src.feature_engineering import CaliforniaHousingFeatureEngineer\n",
    "from src.missing_value_handler import MissingValueHandler\n",
    "\n",
    "# Configure display and plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üè† California Housing Data Processing Environment Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing data\n",
    "print(\"üìä Loading California housing dataset...\")\n",
    "train_data, test_data = load_data_with_fallback()\n",
    "\n",
    "# Validate dataset\n",
    "is_valid = validate_dataset(train_data, test_data)\n",
    "\n",
    "if is_valid:\n",
    "    print(f\"\\n‚úÖ California housing dataset loaded successfully!\")\n",
    "    print(f\"üìà Training data: {train_data.shape}\")\n",
    "    print(f\"üìä Test data: {test_data.shape}\")\n",
    "    print(f\"üéØ Features: {train_data.shape[1] - 1}\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nüìã Dataset Columns:\")\n",
    "    for i, col in enumerate(train_data.columns, 1):\n",
    "        dtype = train_data[col].dtype\n",
    "        print(f\"  {i:2d}. {col:<25} ({dtype})\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset validation failed!\")\n",
    "    raise ValueError(\"Cannot proceed with invalid dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"üìä CALIFORNIA HOUSING DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(train_data):,}\")\n",
    "print(f\"  ‚Ä¢ Total features: {train_data.shape[1] - 1}\")\n",
    "print(f\"  ‚Ä¢ Memory usage: {train_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  ‚Ä¢ Missing values: {train_data.isnull().sum().sum()}\")\n",
    "\n",
    "# Target variable info\n",
    "if 'median_house_value' in train_data.columns:\n",
    "    target = train_data['median_house_value']\n",
    "    print(f\"\\nTarget Variable (median_house_value):\")\n",
    "    print(f\"  ‚Ä¢ Mean: ${target.mean():,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Median: ${target.median():,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Range: ${target.min():,.0f} - ${target.max():,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Standard deviation: ${target.std():,.0f}\")\n",
    "\n",
    "# Feature types\n",
    "print(f\"\\nFeature Types:\")\n",
    "print(f\"  ‚Ä¢ Numerical: {len(train_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  ‚Ä¢ Categorical: {len(train_data.select_dtypes(include=['object', 'string']).columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of target and key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Target distribution\n",
    "if 'median_house_value' in train_data.columns:\n",
    "    target = train_data['median_house_value']\n",
    "    \n",
    "    axes[0,0].hist(target, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('House Value Distribution')\n",
    "    axes[0,0].set_xlabel('Median House Value ($)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log distribution\n",
    "    axes[0,1].hist(np.log1p(target), bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,1].set_title('Log(House Value) Distribution')\n",
    "    axes[0,1].set_xlabel('Log(Median House Value)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Key feature distributions\n",
    "key_features = ['median_income', 'total_rooms', 'housing_median_age', 'population']\n",
    "existing_features = [f for f in key_features if f in train_data.columns]\n",
    "\n",
    "for i, feature in enumerate(existing_features[:4]):\n",
    "    if i < 4:\n",
    "        row = (i + 2) // 3\n",
    "        col = (i + 2) % 3\n",
    "        \n",
    "        train_data[feature].hist(bins=30, ax=axes[row, col], alpha=0.7, color='coral')\n",
    "        axes[row, col].set_title(f'{feature} Distribution')\n",
    "        axes[row, col].set_xlabel(feature)\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the complete preprocessing pipeline\n",
    "print(\"üöÄ Initializing California Housing Preprocessing Pipeline...\")\n",
    "\n",
    "pipeline = CaliforniaHousingPipeline(train_data, test_data)\n",
    "\n",
    "# Show pipeline configuration\n",
    "print(f\"\\n‚öôÔ∏è Pipeline Configuration:\")\n",
    "for key, value in pipeline.pipeline_config.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüîÑ Running complete preprocessing pipeline...\")\n",
    "results = pipeline.run_pipeline(save_processed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature engineering results\n",
    "if pipeline.feature_engineer:\n",
    "    feature_summary = pipeline.feature_engineer.get_feature_summary()\n",
    "    pipeline.feature_engineer.print_feature_summary()\n",
    "    \n",
    "    print(f\"\\nüîç Detailed Feature Analysis:\")\n",
    "    print(f\"  ‚Ä¢ Original California housing features: {feature_summary['original_features']}\")\n",
    "    print(f\"  ‚Ä¢ Engineered features: {feature_summary['total_created']}\")\n",
    "    print(f\"  ‚Ä¢ Total final features: {feature_summary['final_feature_count']}\")\n",
    "    \n",
    "    # Show created features by category\n",
    "    categories = feature_summary['feature_categories']\n",
    "    \n",
    "    for category, features in categories.items():\n",
    "        if features:\n",
    "            print(f\"\\n  üìä {category.title()} Features ({len(features)}):\")\n",
    "            for feature in features:\n",
    "                print(f\"    - {feature}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature engineering not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessing results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original vs processed feature count\n",
    "if 'original_train_shape' in results and 'X_train_shape' in results:\n",
    "    categories = ['Original Features', 'Processed Features']\n",
    "    feature_counts = [results['original_train_shape'][1] - 1, results['feature_count']]\n",
    "    \n",
    "    axes[0,0].bar(categories, feature_counts, color=['lightcoral', 'lightblue'])\n",
    "    axes[0,0].set_title('Feature Count: Before vs After Engineering')\n",
    "    axes[0,0].set_ylabel('Number of Features')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(feature_counts):\n",
    "        axes[0,0].text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Sample size changes\n",
    "if 'original_train_shape' in results and 'X_train_shape' in results:\n",
    "    categories = ['Original Training', 'Final Training', 'Validation']\n",
    "    sample_counts = [\n",
    "        results['original_train_shape'][0],\n",
    "        results['X_train_shape'][0], \n",
    "        results['X_val_shape'][0]\n",
    "    ]\n",
    "    \n",
    "    axes[0,1].bar(categories, sample_counts, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    axes[0,1].set_title('Sample Count: Original vs Final Splits')\n",
    "    axes[0,1].set_ylabel('Number of Samples')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Target distribution before and after processing\n",
    "if pipeline.y_train is not None:\n",
    "    # Original target\n",
    "    original_target = train_data['median_house_value'] if 'median_house_value' in train_data.columns else None\n",
    "    if original_target is not None:\n",
    "        axes[1,0].hist(original_target, bins=30, alpha=0.7, color='skyblue', label='Original')\n",
    "    \n",
    "    # Processed target\n",
    "    axes[1,0].hist(pipeline.y_train, bins=30, alpha=0.7, color='lightcoral', label='Processed')\n",
    "    axes[1,0].set_title('Target Distribution: Original vs Processed')\n",
    "    axes[1,0].set_xlabel('House Value ($)')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].legend()\n",
    "\n",
    "# Feature correlation with target (top 10)\n",
    "if pipeline.X_train is not None and pipeline.y_train is not None:\n",
    "    # Calculate correlations\n",
    "    combined_data = pipeline.X_train.copy()\n",
    "    combined_data[pipeline.target_col] = pipeline.y_train\n",
    "    \n",
    "    correlations = combined_data.corr()[pipeline.target_col].drop(pipeline.target_col)\n",
    "    top_corr = correlations.abs().sort_values(ascending=True).tail(10)\n",
    "    \n",
    "    # Color by positive/negative correlation\n",
    "    colors = ['red' if correlations[feature] < 0 else 'green' for feature in top_corr.index]\n",
    "    \n",
    "    top_corr.plot(kind='barh', ax=axes[1,1], color=colors)\n",
    "    axes[1,1].set_title('Top 10 Features Correlated with House Value')\n",
    "    axes[1,1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual feature engineering steps\n",
    "if pipeline.feature_engineer:\n",
    "    feature_engineer = pipeline.feature_engineer\n",
    "    \n",
    "    print(\"üîç FEATURE ENGINEERING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test individual feature creation methods\n",
    "    print(\"\\nüó∫Ô∏è Testing Geographic Features...\")\n",
    "    temp_train, temp_test = feature_engineer.create_geographic_features()\n",
    "    geo_features = [col for col in temp_train.columns if col not in train_data.columns]\n",
    "    print(f\"Created geographic features: {geo_features}\")\n",
    "    \n",
    "    print(\"\\nüèòÔ∏è Testing Density Features...\")\n",
    "    feature_engineer.train_data = temp_train\n",
    "    feature_engineer.test_data = temp_test\n",
    "    temp_train, temp_test = feature_engineer.create_housing_density_features()\n",
    "    density_features = [col for col in temp_train.columns if col not in feature_engineer.train_data.columns]\n",
    "    print(f\"Created density features: {density_features}\")\n",
    "    \n",
    "    print(\"\\nüí∞ Testing Income Features...\")\n",
    "    feature_engineer.train_data = temp_train\n",
    "    feature_engineer.test_data = temp_test\n",
    "    temp_train, temp_test = feature_engineer.create_income_features()\n",
    "    income_features = [col for col in temp_train.columns if col not in feature_engineer.train_data.columns]\n",
    "    print(f\"Created income features: {income_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the engineered features\n",
    "if pipeline.X_train is not None:\n",
    "    # Plot some key engineered features\n",
    "    engineered_features = ['rooms_per_household', 'population_per_household', 'bedrooms_per_room']\n",
    "    existing_eng_features = [f for f in engineered_features if f in pipeline.X_train.columns]\n",
    "    \n",
    "    if existing_eng_features:\n",
    "        n_features = len(existing_eng_features)\n",
    "        fig, axes = plt.subplots(1, min(3, n_features), figsize=(15, 5))\n",
    "        if n_features == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, feature in enumerate(existing_eng_features[:3]):\n",
    "            if i < len(axes):\n",
    "                pipeline.X_train[feature].hist(bins=30, ax=axes[i], alpha=0.7, color='lightgreen')\n",
    "                axes[i].set_title(f'{feature} Distribution')\n",
    "                axes[i].set_xlabel(feature.replace('_', ' ').title())\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.suptitle('Engineered Feature Distributions')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Geographic features visualization\n",
    "    geo_features = [f for f in pipeline.X_train.columns if 'distance' in f or f in ['is_northern_ca', 'is_coastal']]\n",
    "    \n",
    "    if geo_features:\n",
    "        print(f\"\\nüó∫Ô∏è Geographic Features Created: {len(geo_features)}\")\n",
    "        for feature in geo_features:\n",
    "            print(f\"  ‚Ä¢ {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality After Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data quality after processing\n",
    "if pipeline.X_train is not None:\n",
    "    from src.data_quality import DataQualityAssessor\n",
    "    \n",
    "    # Combine features and target for quality assessment\n",
    "    processed_data = pipeline.X_train.copy()\n",
    "    processed_data[pipeline.target_col] = pipeline.y_train\n",
    "    \n",
    "    quality_assessor = DataQualityAssessor(processed_data, \"Processed California Housing\")\n",
    "    quality_report = quality_assessor.generate_quality_report()\n",
    "    \n",
    "    quality_assessor.print_summary()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Processed data not available for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations after feature engineering\n",
    "if pipeline.X_train is not None and pipeline.y_train is not None:\n",
    "    print(\"üîó FEATURE CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    correlation_data = pipeline.X_train.copy()\n",
    "    correlation_data[pipeline.target_col] = pipeline.y_train\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    target_correlations = correlation_data.corr()[pipeline.target_col].drop(pipeline.target_col)\n",
    "    \n",
    "    # Top positive and negative correlations\n",
    "    top_positive = target_correlations.sort_values(ascending=False).head(10)\n",
    "    top_negative = target_correlations.sort_values(ascending=True).head(5)\n",
    "    \n",
    "    print(f\"\\nüìà Top 10 Positive Correlations:\")\n",
    "    for feature, corr in top_positive.items():\n",
    "        print(f\"  ‚Ä¢ {feature:<30}: {corr:+.3f}\")\n",
    "    \n",
    "    if top_negative.iloc[0] < -0.1:  # Only show if there are meaningful negative correlations\n",
    "        print(f\"\\nüìâ Top Negative Correlations:\")\n",
    "        for feature, corr in top_negative.items():\n",
    "            if corr < -0.1:\n",
    "                print(f\"  ‚Ä¢ {feature:<30}: {corr:+.3f}\")\n",
    "    \n",
    "    # Visualize top correlations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Combine top positive and meaningful negative correlations\n",
    "    top_features = top_positive.head(15)\n",
    "    \n",
    "    colors = ['green' if x > 0 else 'red' for x in top_features.values]\n",
    "    \n",
    "    top_features.plot(kind='barh', color=colors, figsize=(12, 8))\n",
    "    plt.title('Top 15 Features Correlated with House Value')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train-Validation Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the train-validation split\n",
    "if all(x is not None for x in [pipeline.X_train, pipeline.X_val, pipeline.y_train, pipeline.y_val]):\n",
    "    print(\"üìä TRAIN-VALIDATION SPLIT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nSplit Summary:\")\n",
    "    print(f\"  ‚Ä¢ Training samples: {len(pipeline.X_train):,} ({len(pipeline.X_train)/(len(pipeline.X_train)+len(pipeline.X_val))*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Validation samples: {len(pipeline.X_val):,} ({len(pipeline.X_val)/(len(pipeline.X_train)+len(pipeline.X_val))*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Features: {pipeline.X_train.shape[1]}\")\n",
    "    \n",
    "    # Compare target distributions\n",
    "    print(f\"\\nTarget Variable Comparison:\")\n",
    "    print(f\"  Training - Mean: ${pipeline.y_train.mean():,.0f}, Std: ${pipeline.y_train.std():,.0f}\")\n",
    "    print(f\"  Validation - Mean: ${pipeline.y_val.mean():,.0f}, Std: ${pipeline.y_val.std():,.0f}\")\n",
    "    \n",
    "    # Visualize target distribution in both sets\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training target distribution\n",
    "    axes[0].hist(pipeline.y_train, bins=30, alpha=0.7, color='lightblue', label='Training')\n",
    "    axes[0].hist(pipeline.y_val, bins=30, alpha=0.7, color='lightcoral', label='Validation')\n",
    "    axes[0].set_title('Target Distribution: Training vs Validation')\n",
    "    axes[0].set_xlabel('House Value ($)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot comparison\n",
    "    data_for_box = [\n",
    "        pipeline.y_train.values,\n",
    "        pipeline.y_val.values\n",
    "    ]\n",
    "    \n",
    "    axes[1].boxplot(data_for_box, labels=['Training', 'Validation'])\n",
    "    axes[1].set_title('Target Distribution Box Plot')\n",
    "    axes[1].set_ylabel('House Value ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test for distribution similarity\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_statistic, ks_p_value = stats.ks_2samp(pipeline.y_train, pipeline.y_val)\n",
    "    \n",
    "    print(f\"\\nDistribution Similarity Test:\")\n",
    "    print(f\"  ‚Ä¢ KS statistic: {ks_statistic:.4f}\")\n",
    "    print(f\"  ‚Ä¢ P-value: {ks_p_value:.4f}\")\n",
    "    \n",
    "    if ks_p_value > 0.05:\n",
    "        print(f\"  ‚úÖ Distributions are similar (good split)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Distributions may be different\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Split data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Data Preparation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive processing summary\n",
    "processing_summary = pipeline.get_processing_summary()\n",
    "print(processing_summary)\n",
    "\n",
    "# Pipeline information\n",
    "pipeline_info = pipeline.get_pipeline_info()\n",
    "\n",
    "print(f\"\\nüéØ PIPELINE STATUS SUMMARY\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Pipeline fitted: {'‚úÖ Yes' if pipeline_info['is_fitted'] else '‚ùå No'}\")\n",
    "print(f\"Target column: {pipeline_info['target_column']}\")\n",
    "print(f\"Final feature count: {pipeline_info['feature_count']}\")\n",
    "print(f\"Training samples: {pipeline_info['training_samples']:,}\")\n",
    "print(f\"Validation samples: {pipeline_info['validation_samples']:,}\")\n",
    "print(f\"Created features: {pipeline_info['created_features']}\")\n",
    "print(f\"Processing steps completed: {pipeline_info['processing_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Export and Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify processed data is saved and ready for modeling\n",
    "from pathlib import Path\n",
    "from config.settings import PROCESSED_DATA_DIR\n",
    "\n",
    "print(\"üíæ PROCESSED DATA VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what files were saved\n",
    "if PROCESSED_DATA_DIR.exists():\n",
    "    saved_files = list(PROCESSED_DATA_DIR.glob('*.csv')) + list(PROCESSED_DATA_DIR.glob('*.pkl'))\n",
    "    \n",
    "    print(f\"\\nüìÅ Files saved in {PROCESSED_DATA_DIR}:\")\n",
    "    for file_path in saved_files:\n",
    "        file_size = file_path.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"  ‚Ä¢ {file_path.name:<25} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    # Test loading processed data\n",
    "    print(f\"\\nüß™ Testing data loading...\")\n",
    "    try:\n",
    "        loaded_datasets = pipeline.load_processed_data()\n",
    "        \n",
    "        print(f\"  ‚úÖ Successfully loaded:\")\n",
    "        for dataset_name, dataset in loaded_datasets.items():\n",
    "            if isinstance(dataset, pd.DataFrame):\n",
    "                print(f\"    - {dataset_name}: {dataset.shape}\")\n",
    "            elif isinstance(dataset, pd.Series):\n",
    "                print(f\"    - {dataset_name}: ({len(dataset)},)\")\n",
    "            else:\n",
    "                print(f\"    - {dataset_name}: {type(dataset).__name__}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading data: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Processed data directory not found: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing completed and ready for Phase 4: Model Training!\")\n",
    "print(f\"üéØ Next: Train multiple ML models on the processed California housing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide insights for model training phase\n",
    "print(\"üí° KEY INSIGHTS FOR MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if pipeline.X_train is not None and pipeline.y_train is not None:\n",
    "    # Feature importance insights\n",
    "    correlation_data = pipeline.X_train.copy()\n",
    "    correlation_data[pipeline.target_col] = pipeline.y_train\n",
    "    target_corr = correlation_data.corr()[pipeline.target_col].drop(pipeline.target_col)\n",
    "    \n",
    "    strong_features = target_corr[abs(target_corr) > 0.3]\n",
    "    moderate_features = target_corr[(abs(target_corr) > 0.1) & (abs(target_corr) <= 0.3)]\n",
    "    \n",
    "    print(f\"\\nüéØ Modeling Recommendations:\")\n",
    "    \n",
    "    if len(strong_features) > 0:\n",
    "        print(f\"  ‚Ä¢ {len(strong_features)} features with strong correlation (>0.3):\")\n",
    "        for feature, corr in strong_features.abs().sort_values(ascending=False).items():\n",
    "            print(f\"    - {feature}: {target_corr[feature]:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n  ‚Ä¢ Total features available: {pipeline.X_train.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Data is clean and ready for training\")\n",
    "    print(f\"  ‚Ä¢ Recommended models: Linear Regression, Random Forest, XGBoost\")\n",
    "    print(f\"  ‚Ä¢ Target is {'left-skewed' if pipeline.y_train.skew() < -0.5 else 'right-skewed' if pipeline.y_train.skew() > 0.5 else 'approximately normal'}\")\n",
    "    \n",
    "    target_skewness = pipeline.y_train.skew()\n",
    "    if abs(target_skewness) > 1:\n",
    "        print(f\"  ‚Ä¢ Consider log transformation of target (skewness: {target_skewness:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready to proceed to Phase 4: Model Development & Training!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pipeline not completed - cannot provide modeling insights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}